{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the model for Fake News detection over the Buzzfeed-Webis Fake News Corpus 2016. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "from gensim.models import Word2Vec, KeyedVectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to download the Buzzfeed-Webis corpus, which is provided in the form of XML files. read_files will read each file and parse the XML tree to retrieve a tuple of the body of the text ('mainText') and the veracity label ('veracity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_files():\n",
    "    \"\"\"\n",
    "    For each xml file return the main text and the veracity label\n",
    "    \"\"\"\n",
    "    path = 'data/train/'\n",
    "    for filename in os.listdir(path):\n",
    "        if not filename.endswith('.xml'): continue\n",
    "        xmlfile = os.path.join(path, filename)\n",
    "        tree = ET.parse(xmlfile)\n",
    "        yield (tree.find('mainText').text, tree.find('veracity').text)\n",
    "\n",
    "tokenize = lambda doc: doc.lower().split(\" \") # I use the NLTK version, maybe should take this out?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We call this function to get a list of the main text of each article ('documents') as well as a matching list of the labels ('predictions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "documents = [f[0] for f in read_files() if f[0] is not None]\n",
    "possibilities = ['mixture of true and false', 'mostly false', 'no factual content', 'mostly true']\n",
    "predictions = [possibilities.index(f[1]) for f in read_files() if f[0] is not None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we load the Google News pre-trained word embeddings for use in our model. These embeddings are trained using a combination of CBOW and skip-grams over a corpus of over 100 billion words from Google News.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.07910156,  0.12158203, -0.00842285,  0.30664062, -0.15429688,\n",
       "        0.10742188,  0.08398438, -0.0267334 , -0.01831055,  0.1484375 ,\n",
       "       -0.15429688, -0.14160156, -0.21679688,  0.11767578, -0.20605469,\n",
       "        0.1796875 ,  0.42578125,  0.07128906,  0.20117188, -0.19628906,\n",
       "        0.04956055,  0.05932617, -0.09375   ,  0.20898438,  0.01696777,\n",
       "        0.01385498, -0.37109375, -0.00872803, -0.03491211, -0.03320312,\n",
       "        0.10742188, -0.01879883, -0.390625  ,  0.26757812, -0.12158203,\n",
       "        0.08300781, -0.02612305,  0.34960938,  0.12890625,  0.28515625,\n",
       "        0.359375  , -0.01104736,  0.05126953,  0.08300781,  0.05322266,\n",
       "       -0.17285156,  0.14941406,  0.23925781,  0.56640625,  0.19824219,\n",
       "       -0.30078125,  0.17480469, -0.11328125, -0.25976562, -0.31054688,\n",
       "       -0.03088379, -0.49609375, -0.30664062,  0.07763672, -0.21777344,\n",
       "        0.27539062,  0.15039062,  0.22949219,  0.30859375,  0.08154297,\n",
       "       -0.03613281, -0.26953125,  0.14160156, -0.19921875,  0.04199219,\n",
       "        0.00075531, -0.16210938,  0.3515625 ,  0.1328125 , -0.02880859,\n",
       "       -0.20019531, -0.0390625 ,  0.1015625 ,  0.07568359,  0.5234375 ,\n",
       "       -0.28320312, -0.02416992, -0.21289062, -0.07666016,  0.05688477,\n",
       "       -0.05126953,  0.0859375 ,  0.20019531, -0.47265625, -0.0480957 ,\n",
       "        0.31445312, -0.03662109,  0.03857422,  0.21875   ,  0.28710938,\n",
       "       -0.22460938,  0.13867188,  0.2265625 , -0.18652344,  0.06494141,\n",
       "        0.18847656, -0.02734375,  0.23046875, -0.05444336, -0.20996094,\n",
       "       -0.40234375,  0.05273438, -0.171875  , -0.23046875,  0.15820312,\n",
       "        0.26953125,  0.0625    , -0.04394531,  0.26171875,  0.3046875 ,\n",
       "        0.13574219, -0.01519775,  0.1640625 , -0.2578125 ,  0.02416992,\n",
       "       -0.15625   ,  0.08007812,  0.10986328,  0.21972656, -0.04833984,\n",
       "       -0.29492188, -0.00133514,  0.01123047, -0.05029297, -0.34570312,\n",
       "        0.16113281, -0.22753906,  0.02734375,  0.01696777, -0.08007812,\n",
       "        0.11083984, -0.03564453,  0.10791016, -0.10693359, -0.25      ,\n",
       "        0.19140625, -0.34570312, -0.01037598,  0.08154297, -0.07080078,\n",
       "       -0.14257812, -0.23730469, -0.19726562,  0.15136719, -0.02246094,\n",
       "        0.11132812, -0.21777344, -0.01055908, -0.10302734, -0.16796875,\n",
       "        0.08642578, -0.18457031, -0.41796875, -0.1875    ,  0.07714844,\n",
       "       -0.18554688, -0.34179688,  0.05639648, -0.37109375,  0.12255859,\n",
       "        0.01940918, -0.10351562, -0.296875  , -0.07714844,  0.13769531,\n",
       "       -0.3828125 , -0.30859375, -0.04638672, -0.11230469,  0.02514648,\n",
       "       -0.01257324, -0.09521484,  0.04150391,  0.05664062, -0.09423828,\n",
       "       -0.2578125 ,  0.04638672,  0.04492188,  0.03222656, -0.14453125,\n",
       "       -0.19140625, -0.08203125, -0.02856445, -0.35742188,  0.06298828,\n",
       "        0.02941895,  0.05053711, -0.11425781,  0.08740234,  0.01531982,\n",
       "       -0.07373047, -0.07910156, -0.21972656, -0.25195312,  0.04907227,\n",
       "       -0.23535156,  0.24511719, -0.00521851, -0.13476562, -0.515625  ,\n",
       "       -0.11035156,  0.00915527,  0.08300781, -0.14746094,  0.16210938,\n",
       "       -0.06396484,  0.03613281,  0.04614258,  0.16699219, -0.13476562,\n",
       "       -0.1328125 ,  0.19238281, -0.28710938, -0.23046875,  0.29492188,\n",
       "       -0.13085938,  0.21191406, -0.19335938, -0.203125  , -0.14257812,\n",
       "        0.16992188,  0.13085938,  0.04321289,  0.02954102,  0.13769531,\n",
       "        0.13964844,  0.08056641,  0.00671387,  0.06835938,  0.39257812,\n",
       "        0.06030273,  0.07177734, -0.24511719,  0.23632812,  0.05957031,\n",
       "        0.03149414,  0.07226562, -0.04174805,  0.10253906,  0.05078125,\n",
       "        0.08056641, -0.2734375 , -0.03295898, -0.06005859, -0.06201172,\n",
       "       -0.04663086,  0.25390625, -0.2265625 , -0.0300293 ,  0.21972656,\n",
       "        0.25390625, -0.13671875,  0.07519531,  0.00540161,  0.14941406,\n",
       "        0.01153564, -0.07763672, -0.2734375 , -0.06152344, -0.06542969,\n",
       "        0.20507812, -0.14355469, -0.13378906, -0.1875    , -0.06445312,\n",
       "       -0.171875  , -0.10498047, -0.12060547, -0.125     , -0.15429688,\n",
       "       -0.28125   , -0.18359375, -0.14746094, -0.03271484,  0.04248047,\n",
       "        0.16503906,  0.08837891,  0.07568359,  0.01257324,  0.09033203,\n",
       "       -0.08984375, -0.05297852,  0.19921875,  0.15722656, -0.0859375 ,\n",
       "        0.45898438, -0.10644531, -0.09277344,  0.10693359,  0.06005859,\n",
       "       -0.28515625,  0.13183594, -0.39257812,  0.07763672,  0.27148438], dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = 'data/GoogleNews-vectors-negative300.bin'\n",
    "embeddings = KeyedVectors.load_word2vec_format(file, binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To represent entire articles using the Google News word embeddings, we replace each string with it's matching embedding and then taken the elementwise mean of the entire document. This takes a document of N words from being N separate vectors to being a single 1D vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Is there a more efficient way to do this? Looping might be slow for long articles\n",
    "def avg_docvec(docText,embeddings):\n",
    "    \"\"\"\n",
    "    This function converts the text of a document (input as a string) to word embeddings, then\n",
    "    takes the elementwise average of the embeddings to return a single vector.\n",
    "    \"\"\"\n",
    "    docVec = np.zeros(300) # Initialize array for the document\n",
    "    tokens = word_tokenize(doctex) # Creates a list of word tokens (e.g. \"Test words\" -> ['Test', 'words'])\n",
    "    denominator = 0.0 # To take the average, will only count tokens for which we have embeddings in the total  \n",
    "    for token in tokens:\n",
    "        try:\n",
    "            v = embeddings[token]\n",
    "            np.add(docVec,v,out=docVec)\n",
    "            denominator += 1.0\n",
    "        except: # Ignore tokens that aren't in the Google News embeddings\n",
    "            continue\n",
    "    np.divide(docVec,denominator,out=docVec) \n",
    "    return docVec\n",
    "\n",
    "# v = embeddings['Trump']\n",
    "# docVec = np.zeros(300)\n",
    "# np.add(docVec,v,out=docVec)\n",
    "# np.divide(docVec,v,out=docVec)\n",
    "# print(docVec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def docs_to_matrix(documents,embeddings):\n",
    "    \"\"\"\n",
    "    Takes a list of document text strings and returns a matrix of document embeddings.\n",
    "    Note: Does sklearn take matrix inputs in the form (row=samples,columns=elements?)\n",
    "    \"\"\"\n",
    "    matrix = []\n",
    "    for i in range(len(documents)):\n",
    "        vector = avg_docvec(documents[i],embeddings)\n",
    "        if i == 0:\n",
    "            matrix = vector\n",
    "        else:\n",
    "            matrix = np.concatenate((matrix,vector),axis=0) # Concat all vectors into a matrix of order (300,N of docs)\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Calculate TF-IDF over the main text of each article, creating a tf-idf matrix representation of all articles\n",
    "sklearn_tfidf = TfidfVectorizer(norm='l2',min_df=0, use_idf=True, smooth_idf=False, sublinear_tf=True, tokenizer=tokenize)\n",
    "sklearn_representation = sklearn_tfidf.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Splits data into training and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(sklearn_representation, predictions, test_size = .3, random_state=25)\n",
    "LogReg = LogisticRegression()\n",
    "LogReg.fit(X_train, y_train)\n",
    "y_pred = LogReg.predict(X_test)\n",
    "print(y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
